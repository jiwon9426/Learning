<데이터 엔지니어링 기본>
-데이터 플랫폼 :
  . 데이터플랫폼구축(datalake, BI/DW)
  . 데이터분석플랫폼 구축(분석환경)
  . 데이터 거버넌스
-데이터 엔지니어링 :
  . 데이터 파이프라인의 논리 및 아키첵처를 설계하고 구현(데이터수집, 처리, 저장)
  . 분석모델을 운영시스템에 배포하여 Legacy 시스템에 적용
-데이터 엔지니어링 업무 : 1.ETL, 2.Data Modeling, 3.Data Architecturing, 4.시각화, 5.모델배포
-데이터 아키텍처(DA) : 발생하는 데이터와 활용하는 데이터를 어떤 형태로 저장해야하는지 설계. 애플리케이션과 데이터의 연계성이 정의되어야 하며 업무적, 기술적, 체계적 접근을 위한 지식이 복합적으로 필요함
-논리 아케틱처 정의:
  . 원천 -> L0 : 원천 시스템 데이터를 1:1로 수집 / 수집된 데이터와 원천 데이터 일치성 관리해야함 / 정상적인 전송 여부 점검
  . L0   -> L1 : 수집한 데이터의 통합 변환/ 통합된 내용과 수집된 내용의 누락을 점검하고, 기준정보데이터와 삭제 데이터 이력관리해야함 / 기준정보데이터 변환 및 트랜잭션 오류 점검
  . L1   -> L2 : 사용용도에 맞게 데이터 요약처리 / 통합집계데이터와 요약데이터 일치성 점검관리해야함/ 비즈니스 로직처리 점검
-데이터를 정보활용 관점으로 분류 :
  . 디멘전 데이터 : 정보 조회 시, 조회관점이 되는 데이터 ex.고객별, 년도별 매출액에서 고객과 년도
  . 팩트 데이터 : 조회하고자 하는 수치 데이터 ex.고객별, 년도별 매출액에서 매출액
  . 트랜잭션 데이터 : 조회하고자 하는 상세 목록 기준의 데이터 ex.고객별, 년도별 매출액을 집곟는 상세 매출전표리스트에서 매출전표리스트


<DB 빅데이터 아키텍처>
O AWS 빅데이터 아키텍처
-전체 개요 : 
 . 크게 1.관리영역, 2.수집영역, 3.저장영역, 4.처리/분석영역이 있다.
-1. 관리영역 : 
 . airflow가 수집, 처리 Data Pipeline scheduler와 controler역할을 함. 
 . 스케줄에 따라, Airflow package API(=Operator)로 aws 서비스를 액세스하거나, EC2 shell을 실행하거나, custom python module을 실행함
 . airflow? python 언어를 활용해 DAG로 표현되는 workflow를 프로그램적으로 작성하고, 모니터링, 스케줄링하는 플랫폼으로 오픈소스 기반의 워크플로우 스케줄러 시스템임. by아파치
            장점으로는 데이터처리에 익숙한 파이썬기반으로 task를 작성할 수 있고, 에러발생시 분기작업이 가능하고, 콘솔이 따로 존재하여 web UI상에서 관리가 가능함
 . DAG? Directed Asyclic Graph(=비순환 방향 그래프). Task들이 연결되어 하나의 Workflow를 구성함. Task 들의 실행순서를 정의한것이라고 보면 됨.
        1개dag에 여러 task 구성도 되고, 1개 dag에 1개 task 구성도 됨. 다만, task는 내부적으로 종속성을 가지므로 순차적으로 실행되어야하면 한 DAG에서 task로 구분하여 실행해야함.
 . Operator? DAG안에 정의되는 작업 함수. 데이터가 처리되고 이동하는 각각의 개별 Task를 정의하고 있음.
            EX. PythonOperator, PostgresOperator, BranchPythonOperator(조건에 따라 분기를 태워 task를 실행해야할 경우 사용), BashOperator(Bash 쉘에서 명령어를 실행할때)
                SFTPOperator
 . Task? operator가 dag 실행 중 런타임에 호출되면 이를 task라고 표현함. ex.operator가 클래스라면 task는 인스턴스 개념임
         실행조건인 trigger_rule은 디폴트가 all_success이며, 이전의 모든 task가 success일 때 실행됨.
         실행순서를 정의하기 위해서는 task1 >> task2 와 같은 방법으로 할 수 있음.
-2. 수집영역 :
 . Batch : airflow가 source database에 connect해서 데이터를 스토리지에 저장함, 
           airflow가 sftp를 call하면 sftop가 source 스토리지파일을 EC2에 가져옴,
           airflow가 Glue를 call하면 Data lake나 Data hub에 connect해서 데이터를 추출하여 스토리지에 저장함,
           airflow가 S3간 연동을 통해 스토리지에 저장함
-3. 저장영역 :
 . 처리 단계에 따라 데이터를 스토리지에 파일형태로 저장하고 수명주기를 관리함.
 . 스토리지는 Amazon S3이며, 수집/처리/분석 과정에서의 임시파일을 S3저장하기도 하고, L0/L1/L2를 S3에 구성하기도하고, L2를 처리/분석영역의 DW에 저장하기도함
-4. 처리/분석영역 :
 . Processing : spark 기반의 Glue를 통해 S3 데이터를 처리하고, S3 또는 DW에 저장함. 
 . DW : Amazon Redshift를 사용함. L0/L1/L2를 redshift에 구성할 수도 있고, 데이터 변경이 필요한 L2만 redshift에 저장하기도함. Redshift에서 쿼리를 실행하여 처리 또는 분석결과 제공함.
 . Analytics : 데이터분석툴이 DW를 엑세스하여 분석을 수항햄. 분석과정 또는 결과 저장에 S3스토리지를 액세스함
