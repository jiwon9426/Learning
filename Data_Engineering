<데이터 엔지니어링 기본>
-데이터 플랫폼 :
  . 데이터플랫폼구축(datalake, BI/DW)
  . 데이터분석플랫폼 구축(분석환경)
  . 데이터 거버넌스
-데이터 엔지니어링 :
  . 데이터 파이프라인의 논리 및 아키첵처를 설계하고 구현(데이터수집, 처리, 저장)
  . 분석모델을 운영시스템에 배포하여 Legacy 시스템에 적용
-데이터 엔지니어링 업무 : 1.ETL, 2.Data Modeling, 3.Data Architecturing, 4.시각화, 5.모델배포
-데이터 아키텍처(DA) : 발생하는 데이터와 활용하는 데이터를 어떤 형태로 저장해야하는지 설계. 애플리케이션과 데이터의 연계성이 정의되어야 하며 업무적, 기술적, 체계적 접근을 위한 지식이 복합적으로 필요함
-논리 아케틱처 정의:
  . 원천 -> L0 : 원천 시스템 데이터를 1:1로 수집 / 수집된 데이터와 원천 데이터 일치성 관리해야함 / 정상적인 전송 여부 점검
  . L0   -> L1 : 수집한 데이터의 통합 변환/ 통합된 내용과 수집된 내용의 누락을 점검하고, 기준정보데이터와 삭제 데이터 이력관리해야함 / 기준정보데이터 변환 및 트랜잭션 오류 점검
  . L1   -> L2 : 사용용도에 맞게 데이터 요약처리 / 통합집계데이터와 요약데이터 일치성 점검관리해야함/ 비즈니스 로직처리 점검
-데이터를 정보활용 관점으로 분류 :
  . 디멘전 데이터 : 정보 조회 시, 조회관점이 되는 데이터 ex.고객별, 년도별 매출액에서 고객과 년도
  . 팩트 데이터 : 조회하고자 하는 수치 데이터 ex.고객별, 년도별 매출액에서 매출액
  . 트랜잭션 데이터 : 조회하고자 하는 상세 목록 기준의 데이터 ex.고객별, 년도별 매출액을 집곟는 상세 매출전표리스트에서 매출전표리스트


<DB 빅데이터 아키텍처>
O AWS 빅데이터 아키텍처
-전체 개요 : 크게 1.관리영역, 2.수집영역, 3.저장영역, 4.처리/분석영역이 있다.
-1. 관리영역 : 
 . airflow가 수집, 처리 Data Pipeline scheduler와 controler역할을 함. 
 . 스케줄에 따라, Airflow package API(=Operator)로 aws 서비스를 액세스하거나, EC2 shell을 실행하거나, custom python module을 실행함
-2. 수집영역 :
 . Batch : airflow가 source database에 connect해서 데이터를 스토리지에 저장함, 
           airflow가 sftp를 call하면 sftop가 source 스토리지파일을 EC2에 가져옴,
           airflow가 Glue를 call하면 Data lake나 Data hub에 connect해서 데이터를 추출하여 스토리지에 저장함,
           airflow가 S3간 연동을 통해 스토리지에 저장함
-3. 저장영역 :
 . 처리 단계에 따라 데이터를 스토리지에 파일형태로 저장하고 수명주기를 관리함.
 . 스토리지는 Amazon S3이며, 수집/처리/분석 과정에서의 임시파일을 S3저장하기도 하고, L0/L1/L2를 S3에 구성하기도하고, L2를 처리/분석영역의 DW에 저장하기도함
-4. 처리/분석영역 :
 . Processing : spark 기반의 Glue를 통해 S3 데이터를 처리하고, S3 또는 DW에 저장함. 
 . DW : Amazon Redshift를 사용함. L0/L1/L2를 redshift에 구성할 수도 있고, 데이터 변경이 필요한 L2만 redshift에 저장하기도함. Redshift에서 쿼리를 실행하여 처리 또는 분석결과 제공함.
 . Analytics : 데이터분석툴이 DW를 엑세스하여 분석을 수항햄. 분석과정 또는 결과 저장에 S3스토리지를 액세스함
