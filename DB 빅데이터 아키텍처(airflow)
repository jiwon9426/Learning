O AWS 빅데이터 아키텍처

-전체 개요 : 
 . 크게 1.관리영역, 2.수집영역, 3.저장영역, 4.처리/분석영역이 있다.

-1. 관리영역 : 
 . airflow가 수집, 처리 Data Pipeline scheduler와 controler역할을 함. 
 . 스케줄에 따라, Airflow package API(=Operator)로 aws 서비스를 액세스하거나, EC2 shell을 실행하거나, custom python module을 실행함
 . airflow? python 언어를 활용해 DAG로 표현되는 workflow를 프로그램적으로 작성하고, 모니터링, 스케줄링하는 플랫폼으로 오픈소스 기반의 워크플로우 스케줄러 시스템임. by아파치
            장점으로는 데이터처리에 익숙한 파이썬기반으로 task를 작성할 수 있고, 에러발생시 분기작업이 가능하고, 콘솔이 따로 존재하여 web UI상에서 관리가 가능함
 . DAG? Directed Asyclic Graph(=비순환 방향 그래프). Task들이 연결되어 하나의 Workflow를 구성함. Task 들의 실행순서를 정의한것이라고 보면 됨.
        1개dag에 여러 task 구성도 되고, 1개 dag에 1개 task 구성도 됨. 다만, task는 내부적으로 종속성을 가지므로 순차적으로 실행되어야하면 한 DAG에서 task로 구분하여 실행해야함.
 . Operator? DAG안에 정의되는 작업 함수. 데이터가 처리되고 이동하는 각각의 개별 Task를 정의하고 있음.
            EX. PythonOperator, PostgresOperator, BranchPythonOperator(조건에 따라 분기를 태워 task를 실행해야할 경우 사용), BashOperator(Bash 쉘에서 명령어를 실행할때)
                SFTPOperator
 . Task? operator가 dag 실행 중 런타임에 호출되면 이를 task라고 표현함. ex.operator가 클래스라면 task는 인스턴스 개념임
         실행조건인 trigger_rule은 디폴트가 all_success이며, 이전의 모든 task가 success일 때 실행됨.
         실행순서를 정의하기 위해서는 task1 >> task2 와 같은 방법으로 할 수 있음.
         
-2. 수집영역 :
 . 크게 Batch와 Realtime 수집설계로 나뉜다. Batch는 airflow의 phthon API(=operator)로 수행하고, real time 수집은 Kinesis를 사용함
 . Batch : airflow가 source database에 connect해서 데이터를 스토리지에 저장함, =database에 conncet하여 테이블에 접근한 뒤, file 형태로 S3에 적재함
           airflow가 sftp를 call하면 sftop가 source 스토리지파일을 EC2에 가져옴,
           airflow가 Glue를 call하면 Data lake나 Data hub에 connect해서 데이터를 추출하여 스토리지에 저장함,
           airflow가 S3간 연동을 통해 스토리지에 저장함
 . Realtime 수집 : Amazon Kinesis가 대표적(redshift, dynamoDB, Lambda등과 같은 다른 AWS 서비스와 통합하고 스트리밍 데이터수집과정을 단순화하였음
                                          /실시간으로 스트리밍 데이터를 수집, 버퍼링 및 처리하고 또한 완전관리형 스트리밍 애플리케이션을 운영하여 인프라를 관리할 필요가 없음)
                   Amazon Kinesis Data Streams? 실시간 분석을 위해 대규모로 스트리밍 데이터를 수집하기 위해 사용. 데이터가 수신되는 대로 처리 및 분석이 가능함.
                                                for 데이터 스트림을 수집하여 처리분석하는 실시간 애플리케이션을 구축하고 싶을 때
                   Amazon Kinesis Data Firehose? 스트리밍 데이터를 Data Lake, 데이터 스토어 및 분석도구에 로드하기 위해 사용.
                                                for 데이터 스트림을 캡처 및 변환하여 AWS 데이터 스토어로 로드하고 싶을때
                  -> 둘 중에 맞는 것 선택하면 됨.
                  
-3. 저장영역 :
 . 처리 단계에 따라 데이터를 스토리지에 파일형태로 저장하고 수명주기를 관리함.
 . 스토리지는 Amazon S3이며, 수집/처리/분석 과정에서의 임시파일을 S3저장하기도 하고, L0/L1/L2를 S3에 구성하기도하고, L2를 처리/분석영역의 DW에 저장하기도함
 . S3기반으로 파이프라인을 구축하는데, 데이터수집영역(Batch 및 Real time)과 저장(S3), 데이터처리/분석(AWS Glue)와 DW영역(Amazon Redshift)와 연결성을 지님
 . S3는 데이터수집 목적으로 Data target 영역에 위치시킬 수도 있고, 데이터처리/분석을 위한 DW 목적으로 Data source 영역에 위치시킬 수도 있음
 . bucket 네임 룰이 있음 {서비스}-{고객사}-{리전}-{구축시스템}-{목적}-{데이터형태}-{layer 구분}-{원천시스템}
 . Life cycle 관리기준 : 보관 대상을 정의하여 해당 주기에 따라,
                        hot(빈번한 데이터 참조와 변경 발생 ex.redshift) 
                        -> warm (간헐적인 데이터참조 발생) 
                        -> cold(데이터참조 거의 발생x, but 영구적 보관 필요) 저장소 유형으로 변경되도록 관리한다.
 . S3 버전관리: S3 버킷 버져닝(versioning)을 활성화할 경우, 삭제와 변경에 대하여 버전관리가 가능하며 과거시점으로 복원이 가능함.
               S3 버킷은 버전 관리 활성화를 기본으로 함. 단, 데이터의 개발영역은 버전 관리를 비활성화함. 

-4. 처리/분석영역 :
 . Processing : spark 기반의 Glue를 통해 S3 데이터를 처리하고, S3 또는 DW에 저장함. 
 . DW : Amazon Redshift를 사용함. L0/L1/L2를 redshift에 구성할 수도 있고, 데이터 변경이 필요한 L2만 redshift에 저장하기도함. Redshift에서 쿼리를 실행하여 처리 또는 분석결과 제공함.
 . Analytics : 데이터분석툴이 DW를 엑세스하여 분석을 수항햄. 분석과정 또는 결과 저장에 S3스토리지를 액세스함
 . AWS Glue는 서비리스 ETL 서비스로서 데이터를 손쉽게 구성하고 로드할 수 있음
 . 메타데이터(=테이블스키마)는 glue data catalog에 등록하여 관리함. 이렇게 s3에 저장한 데이터셋에 대한 data catalog를 통해 glue가 워크플로우를 처리함.
 . L0 데이터 처리: 1.초기 데이터 적재(최종 테이블에 데이터가 존재한다며 삭제 후 데이터를 적재하는 로직을 구현) 
                  2.증분 데이터 적재(데이터를 비교하여 생성한 증분데이터를 delete/insert 방식으로 적재하는 로직을 구현)
 . L1/L2 데이터 처리: L1(비정형데이터의 전처리과정으로 서로 다른 포맷의 데이터정제와 매칭을 수행함)
                     L2(L1데이털부터 분석 목적에 맞는 데이터를 생성하여 저장함. 외부에서 분석결과데이터를 수신한 경우, L2에 저장함)
 . DW : 대량 원천 데이터의 저장, 통합, 변환, 분석 sql 동시병렬수행, olap 엑세스 수용 등등 처리/분석의 한 영역임
 . Amazon Athena: S3의 데이터를 SQL을 통해 분석할 수 있는 대화형 쿼리 서비스. 
                  서버리스 서비스로서, 쿼리를 병렬로 실행하여 자동으로 확장하므로 대규모 데이터셋에 대한 복잡한 쿼리의 경우에도 빠르게 실행결과를 얻을 수 있음.
                  이를 사용하면 S3에 저장된 비정형, 반정형 정형 데이터를 분석할 수 있음
                  또한 csv, json, apache parquet, apache ORC 같은 컬럼 형식의 데이터를 집계하거나 로드할 필요없이 ANSI sql을 사용한 쿼리를 실행할 수 있음.
                  AWS Glue를 통해 athena에서 테이블과 쿼리 데이터를 생성하고 aws glue 및 데이터 검색기능을 통합할 수 있음
